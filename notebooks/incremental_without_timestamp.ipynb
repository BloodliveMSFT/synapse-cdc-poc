{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Incremental Ingestion Without Timestamp (Hash-Based CDC)\n",
        "\n",
        "This notebook demonstrates CDC-style incremental ingestion from CSV files **without a timestamp column** using a **row hash comparison** approach.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Load** source CSV files\n",
        "2. **Generate** a hash for each row based on all columns\n",
        "3. **Compare** with previously processed hashes stored in metadata\n",
        "4. **Identify** new and changed records\n",
        "5. **Write** incremental records to destination\n",
        "6. **Update** the hash registry for next run\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- CSV files (without timestamp) uploaded to `/data/source/`\n",
        "- Storage account configured with Synapse workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Update the storage account name below to match your deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Update these values for your environment\n",
        "# ============================================================================\n",
        "\n",
        "# Storage account name (from deployment output)\n",
        "STORAGE_ACCOUNT = \"<your-storage-account-name>\"\n",
        "\n",
        "# Container name\n",
        "CONTAINER = \"data\"\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
        "SOURCE_PATH = f\"{BASE_PATH}/source/\"\n",
        "DESTINATION_PATH = f\"{BASE_PATH}/destination/without_timestamp/\"\n",
        "METADATA_PATH = f\"{BASE_PATH}/metadata/\"\n",
        "\n",
        "# Hash registry file (stores previously processed record hashes)\n",
        "HASH_REGISTRY_FILE = f\"{METADATA_PATH}hash_registry/\"\n",
        "\n",
        "# Source file pattern (files without timestamp column)\n",
        "SOURCE_FILE_PATTERN = \"products_no_ts*.csv\"\n",
        "\n",
        "# Primary key column for identifying records\n",
        "PRIMARY_KEY_COLUMN = \"product_id\"\n",
        "\n",
        "print(f\"Source Path: {SOURCE_PATH}\")\n",
        "print(f\"Destination Path: {DESTINATION_PATH}\")\n",
        "print(f\"Metadata Path: {METADATA_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Source Data\n",
        "\n",
        "Read all CSV files from the source folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, concat_ws, md5, lit, current_timestamp,\n",
        "    when, coalesce\n",
        ")\n",
        "from pyspark.sql.types import StringType\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Spark session (already available in Synapse)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Load source CSV files\n",
        "source_file_path = f\"{SOURCE_PATH}{SOURCE_FILE_PATTERN}\"\n",
        "print(f\"Loading source files from: {source_file_path}\")\n",
        "\n",
        "try:\n",
        "    # Read CSV files with header\n",
        "    source_df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .csv(source_file_path)\n",
        "    \n",
        "    total_records = source_df.count()\n",
        "    print(f\"\\nTotal records in source: {total_records}\")\n",
        "    print(f\"\\nSource Schema:\")\n",
        "    source_df.printSchema()\n",
        "    print(f\"\\nSample source data:\")\n",
        "    source_df.show(5, truncate=False)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading source files: {str(e)}\")\n",
        "    print(\"Make sure CSV files are uploaded to the source folder.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Generate Row Hashes\n",
        "\n",
        "Create a unique hash for each row based on all column values. This hash will be used to detect changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_row_hash(df, exclude_columns=None):\n",
        "    \"\"\"\n",
        "    Generate MD5 hash for each row based on all columns (except excluded ones).\n",
        "    \"\"\"\n",
        "    if exclude_columns is None:\n",
        "        exclude_columns = []\n",
        "    \n",
        "    # Get columns to hash (exclude specified columns)\n",
        "    hash_columns = [c for c in df.columns if c not in exclude_columns]\n",
        "    \n",
        "    # Convert all columns to string and concatenate\n",
        "    # Use coalesce to handle nulls\n",
        "    concat_expr = concat_ws(\n",
        "        \"|\",\n",
        "        *[coalesce(col(c).cast(StringType()), lit(\"NULL\")) for c in hash_columns]\n",
        "    )\n",
        "    \n",
        "    # Generate MD5 hash\n",
        "    return df.withColumn(\"_row_hash\", md5(concat_expr))\n",
        "\n",
        "# Generate hashes for source data\n",
        "source_with_hash = generate_row_hash(source_df)\n",
        "\n",
        "print(\"Source data with row hashes:\")\n",
        "source_with_hash.select(PRIMARY_KEY_COLUMN, \"_row_hash\").show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Previous Hash Registry\n",
        "\n",
        "Read the hash registry from previous runs to compare against."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_hash_registry():\n",
        "    \"\"\"\n",
        "    Load the hash registry from previous runs.\n",
        "    Returns empty DataFrame if no registry exists (first run).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        registry_df = spark.read.parquet(HASH_REGISTRY_FILE)\n",
        "        count = registry_df.count()\n",
        "        print(f\"Loaded existing hash registry with {count} records.\")\n",
        "        return registry_df\n",
        "    except Exception as e:\n",
        "        print(f\"No existing hash registry found. This appears to be the first run.\")\n",
        "        # Return empty DataFrame with expected schema\n",
        "        from pyspark.sql.types import StructType, StructField, StringType\n",
        "        schema = StructType([\n",
        "            StructField(PRIMARY_KEY_COLUMN, StringType(), True),\n",
        "            StructField(\"_row_hash\", StringType(), True),\n",
        "            StructField(\"_registered_at\", StringType(), True)\n",
        "        ])\n",
        "        return spark.createDataFrame([], schema)\n",
        "\n",
        "# Load previous hash registry\n",
        "previous_registry = load_hash_registry()\n",
        "previous_count = previous_registry.count()\n",
        "print(f\"\\nPrevious registry records: {previous_count}\")\n",
        "\n",
        "if previous_count > 0:\n",
        "    print(\"\\nPrevious registry sample:\")\n",
        "    previous_registry.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Identify New and Changed Records\n",
        "\n",
        "Compare current hashes with previous registry to find:\n",
        "- **New records**: Primary key not in previous registry\n",
        "- **Changed records**: Primary key exists but hash is different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def identify_incremental_records(current_df, previous_registry_df, pk_column):\n",
        "    \"\"\"\n",
        "    Identify new and changed records by comparing hashes.\n",
        "    Returns DataFrame with incremental records and change type.\n",
        "    \"\"\"\n",
        "    # Rename columns in previous registry to avoid conflicts\n",
        "    prev_renamed = previous_registry_df.select(\n",
        "        col(pk_column).alias(f\"_prev_{pk_column}\"),\n",
        "        col(\"_row_hash\").alias(\"_prev_hash\")\n",
        "    )\n",
        "    \n",
        "    # Left join current with previous\n",
        "    joined = current_df.join(\n",
        "        prev_renamed,\n",
        "        current_df[pk_column] == prev_renamed[f\"_prev_{pk_column}\"],\n",
        "        \"left\"\n",
        "    )\n",
        "    \n",
        "    # Identify change type\n",
        "    # - NEW: no previous record (prev_pk is null)\n",
        "    # - CHANGED: previous record exists but hash is different\n",
        "    # - UNCHANGED: previous record exists and hash is same\n",
        "    result = joined.withColumn(\n",
        "        \"_change_type\",\n",
        "        when(col(f\"_prev_{pk_column}\").isNull(), lit(\"NEW\"))\n",
        "        .when(col(\"_row_hash\") != col(\"_prev_hash\"), lit(\"CHANGED\"))\n",
        "        .otherwise(lit(\"UNCHANGED\"))\n",
        "    )\n",
        "    \n",
        "    # Filter to only new and changed records\n",
        "    incremental = result.filter(col(\"_change_type\") != \"UNCHANGED\")\n",
        "    \n",
        "    # Drop temporary columns\n",
        "    incremental = incremental.drop(f\"_prev_{pk_column}\", \"_prev_hash\")\n",
        "    \n",
        "    return incremental\n",
        "\n",
        "# Identify incremental records\n",
        "incremental_df = identify_incremental_records(\n",
        "    source_with_hash, \n",
        "    previous_registry, \n",
        "    PRIMARY_KEY_COLUMN\n",
        ")\n",
        "\n",
        "# Count by change type\n",
        "print(\"Change Summary:\")\n",
        "incremental_df.groupBy(\"_change_type\").count().show()\n",
        "\n",
        "incremental_count = incremental_df.count()\n",
        "print(f\"\\nTotal incremental records: {incremental_count}\")\n",
        "\n",
        "if incremental_count > 0:\n",
        "    print(\"\\nIncremental records:\")\n",
        "    incremental_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Write Incremental Data to Destination\n",
        "\n",
        "Append the new/changed records to the destination folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if incremental_count > 0:\n",
        "    # Add processing metadata\n",
        "    output_df = incremental_df.withColumn(\"_processed_at\", current_timestamp())\n",
        "    \n",
        "    # Select columns for output (include change type for tracking)\n",
        "    output_columns = [c for c in source_df.columns] + [\"_change_type\", \"_processed_at\"]\n",
        "    output_df = output_df.select(output_columns)\n",
        "    \n",
        "    # Write to destination (append mode)\n",
        "    print(f\"Writing {incremental_count} records to: {DESTINATION_PATH}\")\n",
        "    \n",
        "    output_df.write \\\n",
        "        .mode(\"append\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(DESTINATION_PATH)\n",
        "    \n",
        "    print(f\"\\nSuccessfully wrote {incremental_count} records to destination.\")\n",
        "    print(\"\\nOutput sample:\")\n",
        "    output_df.show(5, truncate=False)\n",
        "else:\n",
        "    print(\"No new or changed records to write. Skipping write operation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Update Hash Registry\n",
        "\n",
        "Save the current hashes to the registry for the next run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_hash_registry(current_df, pk_column):\n",
        "    \"\"\"\n",
        "    Update the hash registry with current record hashes.\n",
        "    This overwrites the previous registry with the complete current state.\n",
        "    \"\"\"\n",
        "    # Create registry with primary key, hash, and timestamp\n",
        "    registry_df = current_df.select(\n",
        "        col(pk_column),\n",
        "        col(\"_row_hash\"),\n",
        "        current_timestamp().alias(\"_registered_at\")\n",
        "    )\n",
        "    \n",
        "    # Write registry (overwrite mode - full snapshot)\n",
        "    registry_df.write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .parquet(HASH_REGISTRY_FILE)\n",
        "    \n",
        "    count = registry_df.count()\n",
        "    print(f\"Hash registry updated with {count} records.\")\n",
        "    return count\n",
        "\n",
        "# Update hash registry with current state\n",
        "registry_count = update_hash_registry(source_with_hash, PRIMARY_KEY_COLUMN)\n",
        "print(f\"\\nRegistry now contains {registry_count} record hashes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verify Results\n",
        "\n",
        "Check the destination folder and current hash registry state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read and display destination data\n",
        "print(\"=\" * 60)\n",
        "print(\"VERIFICATION: Current State\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    dest_df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(DESTINATION_PATH)\n",
        "    \n",
        "    dest_count = dest_df.count()\n",
        "    print(f\"\\nTotal records in destination: {dest_count}\")\n",
        "    \n",
        "    # Show change type distribution\n",
        "    print(\"\\nRecords by change type:\")\n",
        "    dest_df.groupBy(\"_change_type\").count().show()\n",
        "    \n",
        "    print(f\"\\nDestination data sample:\")\n",
        "    dest_df.show(10, truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"No data in destination yet or error reading: {str(e)}\")\n",
        "\n",
        "# Read current hash registry\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"Current Hash Registry State:\")\n",
        "print(\"=\" * 60)\n",
        "try:\n",
        "    registry_df = spark.read.parquet(HASH_REGISTRY_FILE)\n",
        "    print(f\"\\nRegistry contains {registry_df.count()} records.\")\n",
        "    registry_df.show(10, truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"No hash registry found: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Hash-based CDC**: Using MD5 hashes to detect changes without timestamps\n",
        "2. **Change Detection**: Identifying NEW and CHANGED records\n",
        "3. **State Persistence**: Hash registry stored for subsequent runs\n",
        "\n",
        "### To Test Incremental Behavior:\n",
        "\n",
        "1. Run this notebook with the initial dataset\n",
        "2. Upload a new CSV file with:\n",
        "   - New records (new product_id values)\n",
        "   - Modified records (existing product_id with changed values)\n",
        "3. Run the notebook again - only new/changed records will be processed\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "- The hash registry is stored as Parquet in `/data/metadata/hash_registry/`\n",
        "- Each run appends new data to `/data/destination/without_timestamp/`\n",
        "- The `_change_type` column indicates whether a record is NEW or CHANGED\n",
        "- The `_processed_at` column tracks when records were ingested\n",
        "\n",
        "### Comparison with Timestamp-Based Approach:\n",
        "\n",
        "| Aspect | Timestamp-Based | Hash-Based |\n",
        "|--------|-----------------|------------|\n",
        "| Requires timestamp column | Yes | No |\n",
        "| Detects any field change | Only if timestamp updated | Yes |\n",
        "| Storage overhead | Minimal (single value) | Higher (hash per record) |\n",
        "| Processing complexity | Lower | Higher |"
      ]
    }
  ]
}

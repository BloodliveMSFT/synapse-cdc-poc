{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Incremental Ingestion with Timestamp (CDC Pattern)\n",
        "\n",
        "This notebook demonstrates CDC-style incremental ingestion from CSV files using a **timestamp-based watermark** approach.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Read** the last processed watermark timestamp from metadata\n",
        "2. **Load** source CSV files\n",
        "3. **Filter** records newer than the watermark\n",
        "4. **Write** new/changed records to destination\n",
        "5. **Update** the watermark with the latest timestamp\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- CSV files with a `last_updated_ts` column uploaded to `/data/source/`\n",
        "- Storage account configured with Synapse workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Update the storage account name below to match your deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Update these values for your environment\n",
        "# ============================================================================\n",
        "\n",
        "# Storage account name (from deployment output)\n",
        "STORAGE_ACCOUNT = \"<your-storage-account-name>\"\n",
        "\n",
        "# Container name\n",
        "CONTAINER = \"data\"\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
        "SOURCE_PATH = f\"{BASE_PATH}/source/\"\n",
        "DESTINATION_PATH = f\"{BASE_PATH}/destination/with_timestamp/\"\n",
        "METADATA_PATH = f\"{BASE_PATH}/metadata/\"\n",
        "\n",
        "# Watermark file\n",
        "WATERMARK_FILE = f\"{METADATA_PATH}watermark_timestamp.json\"\n",
        "\n",
        "# Source file pattern (files with timestamp column)\n",
        "SOURCE_FILE_PATTERN = \"customers_with_ts*.csv\"\n",
        "\n",
        "# Timestamp column name in source data\n",
        "TIMESTAMP_COLUMN = \"last_updated_ts\"\n",
        "\n",
        "# Primary key column for identifying records\n",
        "PRIMARY_KEY_COLUMN = \"customer_id\"\n",
        "\n",
        "print(f\"Source Path: {SOURCE_PATH}\")\n",
        "print(f\"Destination Path: {DESTINATION_PATH}\")\n",
        "print(f\"Metadata Path: {METADATA_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Read Current Watermark\n",
        "\n",
        "The watermark stores the last processed timestamp. On first run, it will be initialized to a minimum date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max as spark_max, lit, to_timestamp\n",
        "\n",
        "# Initialize Spark session (already available in Synapse)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "def read_watermark():\n",
        "    \"\"\"\n",
        "    Read the current watermark timestamp from metadata.\n",
        "    Returns minimum date if watermark doesn't exist (first run).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to read existing watermark\n",
        "        watermark_df = spark.read.json(WATERMARK_FILE)\n",
        "        watermark_value = watermark_df.select(\"last_watermark\").collect()[0][0]\n",
        "        print(f\"Existing watermark found: {watermark_value}\")\n",
        "        return watermark_value\n",
        "    except Exception as e:\n",
        "        # First run - no watermark exists\n",
        "        default_watermark = \"1900-01-01 00:00:00\"\n",
        "        print(f\"No existing watermark found. Using default: {default_watermark}\")\n",
        "        return default_watermark\n",
        "\n",
        "# Read current watermark\n",
        "current_watermark = read_watermark()\n",
        "print(f\"\\nCurrent Watermark: {current_watermark}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Source Data\n",
        "\n",
        "Read all CSV files from the source folder that match our pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load source CSV files\n",
        "source_file_path = f\"{SOURCE_PATH}{SOURCE_FILE_PATTERN}\"\n",
        "print(f\"Loading source files from: {source_file_path}\")\n",
        "\n",
        "try:\n",
        "    # Read CSV files with header\n",
        "    source_df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .csv(source_file_path)\n",
        "    \n",
        "    # Convert timestamp column to proper timestamp type\n",
        "    source_df = source_df.withColumn(\n",
        "        TIMESTAMP_COLUMN, \n",
        "        to_timestamp(col(TIMESTAMP_COLUMN), \"yyyy-MM-dd HH:mm:ss\")\n",
        "    )\n",
        "    \n",
        "    total_records = source_df.count()\n",
        "    print(f\"\\nTotal records in source: {total_records}\")\n",
        "    print(f\"\\nSource Schema:\")\n",
        "    source_df.printSchema()\n",
        "    print(f\"\\nSample source data:\")\n",
        "    source_df.show(5, truncate=False)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading source files: {str(e)}\")\n",
        "    print(\"Make sure CSV files are uploaded to the source folder.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Filter Incremental Records\n",
        "\n",
        "Select only records where the timestamp is greater than the current watermark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter records newer than the watermark\n",
        "incremental_df = source_df.filter(\n",
        "    col(TIMESTAMP_COLUMN) > to_timestamp(lit(current_watermark), \"yyyy-MM-dd HH:mm:ss\")\n",
        ")\n",
        "\n",
        "incremental_count = incremental_df.count()\n",
        "print(f\"Records newer than watermark ({current_watermark}): {incremental_count}\")\n",
        "\n",
        "if incremental_count > 0:\n",
        "    print(f\"\\nIncremental records to process:\")\n",
        "    incremental_df.orderBy(col(TIMESTAMP_COLUMN)).show(truncate=False)\n",
        "else:\n",
        "    print(\"\\nNo new records to process. Data is up to date.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Write Incremental Data to Destination\n",
        "\n",
        "Append the new/changed records to the destination folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if incremental_count > 0:\n",
        "    # Add processing metadata\n",
        "    from pyspark.sql.functions import current_timestamp\n",
        "    \n",
        "    output_df = incremental_df.withColumn(\"_processed_at\", current_timestamp())\n",
        "    \n",
        "    # Write to destination (append mode)\n",
        "    output_path = f\"{DESTINATION_PATH}\"\n",
        "    print(f\"Writing {incremental_count} records to: {output_path}\")\n",
        "    \n",
        "    output_df.write \\\n",
        "        .mode(\"append\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(output_path)\n",
        "    \n",
        "    print(f\"\\nSuccessfully wrote {incremental_count} records to destination.\")\n",
        "else:\n",
        "    print(\"No records to write. Skipping write operation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Update Watermark\n",
        "\n",
        "Save the new watermark (maximum timestamp from processed records)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_watermark(df, timestamp_column):\n",
        "    \"\"\"\n",
        "    Update the watermark with the maximum timestamp from the processed data.\n",
        "    \"\"\"\n",
        "    # Get the maximum timestamp from the data\n",
        "    max_timestamp = df.agg(spark_max(col(timestamp_column))).collect()[0][0]\n",
        "    \n",
        "    if max_timestamp:\n",
        "        # Format timestamp as string\n",
        "        new_watermark = max_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        \n",
        "        # Create watermark record\n",
        "        watermark_data = [{\n",
        "            \"last_watermark\": new_watermark,\n",
        "            \"updated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"records_processed\": df.count()\n",
        "        }]\n",
        "        \n",
        "        # Write watermark to metadata\n",
        "        watermark_df = spark.createDataFrame(watermark_data)\n",
        "        watermark_df.write.mode(\"overwrite\").json(WATERMARK_FILE)\n",
        "        \n",
        "        print(f\"Watermark updated to: {new_watermark}\")\n",
        "        return new_watermark\n",
        "    else:\n",
        "        print(\"No timestamp found. Watermark not updated.\")\n",
        "        return None\n",
        "\n",
        "# Update watermark if we processed records\n",
        "if incremental_count > 0:\n",
        "    new_watermark = update_watermark(incremental_df, TIMESTAMP_COLUMN)\n",
        "else:\n",
        "    print(\"No records processed. Watermark remains unchanged.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Verify Results\n",
        "\n",
        "Check the destination folder and current watermark state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read and display destination data\n",
        "print(\"=\" * 60)\n",
        "print(\"VERIFICATION: Current State\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    dest_df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(DESTINATION_PATH)\n",
        "    \n",
        "    dest_count = dest_df.count()\n",
        "    print(f\"\\nTotal records in destination: {dest_count}\")\n",
        "    print(f\"\\nDestination data sample:\")\n",
        "    dest_df.show(10, truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"No data in destination yet or error reading: {str(e)}\")\n",
        "\n",
        "# Read current watermark\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"Current Watermark State:\")\n",
        "print(\"=\" * 60)\n",
        "try:\n",
        "    watermark_df = spark.read.json(WATERMARK_FILE)\n",
        "    watermark_df.show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"No watermark file found: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Watermark-based CDC**: Using a timestamp column to track processed records\n",
        "2. **Incremental Processing**: Only new/changed records are processed\n",
        "3. **State Persistence**: Watermark stored in metadata for subsequent runs\n",
        "\n",
        "### To Test Incremental Behavior:\n",
        "\n",
        "1. Run this notebook with the initial dataset\n",
        "2. Upload a new CSV file with records having newer timestamps\n",
        "3. Run the notebook again - only new records will be processed\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "- The watermark is stored as a JSON file in `/data/metadata/`\n",
        "- Each run appends new data to `/data/destination/with_timestamp/`\n",
        "- The `_processed_at` column tracks when records were ingested"
      ]
    }
  ]
}
